{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Linear Models and Validation Metrics</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Red'>In this project, we will write code that uses linear models to perform classification and regression tasks. We  will also be describing the process of how code was written. More details can be found below. Any websites or AI tools used will be cited, if used.</font>\n",
        "\n",
        "NOTE: You may use the Table of Content on the left side of this notebook to efficiently navigate within this document/project.\n",
        "\n",
        "---\n",
        "\n",
        "|                **Question**                |\n",
        "|:------------------------------------------:|\n",
        "|         **Part 1: Classification**         |\n",
        "|          Step 0: Import Libraries          |\n",
        "|             Step 1: Data Input             |\n",
        "|           Step 2: Data Processing          |\n",
        "| Step 3: Implement Machine Learning Model   |\n",
        "|           Step 4: Validate Model           |\n",
        "|          Step 5: Visualize Results         |\n",
        "|                  Questions                 |\n",
        "|             Process Description            |\n",
        "|           **Part 2: Regression**           |\n",
        "|             Step 1: Data Input             |\n",
        "|           Step 2: Data Processing          |\n",
        "| Step 3: Implement Machine Learning Model   |\n",
        "|            Step 4: Validate Mode           |\n",
        "|          Step 5: Visualize Results         |\n",
        "|                  Questions                 |\n",
        "|             Process Description            |\n",
        "|  **Part 3:   Observations/Interpretation** |\n",
        "|  **Part 4: Lasso and Ridge regression**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 1: Classification**\n",
        "\n",
        "|                **Question**                |\n",
        "|:------------------------------------------:|\n",
        "|         **Part 1: Classification**         |\n",
        "|          Step 0: Import Libraries          |\n",
        "|             Step 1: Data Input             |\n",
        "|           Step 2: Data Processing          |\n",
        "| Step 3: Implement Machine Learning   Model |\n",
        "|           Step 4: Validate Model           |\n",
        "|          Step 5: Visualize Results         |\n",
        "|                  Questions                 |\n",
        "|             Process Description            |\n",
        "\n",
        "Developing code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow, the relevant code in each of the steps below is written/coded:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "## **Step 0:** Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Using the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y` and then printing the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of X: (4600, 57)\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y: (4600,)\n",
            "Type of y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Import the load_spam function from yellowbrick.datasets\n",
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "# Load the spam dataset\n",
        "X, y = load_spam()\n",
        "\n",
        "# Print the size and type of X and y\n",
        "print(\"Size of X:\", X.shape)\n",
        "print(\"Type of X:\", type(X))\n",
        "print(\"Size of y:\", y.shape)\n",
        "print(\"Type of y:\", type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing\n",
        "\n",
        "Checking to see if there are any missing values in the dataset. If necessary, selecting an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "4e7204f5",
      "metadata": {
        "id": "4e7204f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values in X:\n",
            "\n",
            " word_freq_make                0\n",
            "word_freq_address             0\n",
            "word_freq_all                 0\n",
            "word_freq_3d                  0\n",
            "word_freq_our                 0\n",
            "word_freq_over                0\n",
            "word_freq_remove              0\n",
            "word_freq_internet            0\n",
            "word_freq_order               0\n",
            "word_freq_mail                0\n",
            "word_freq_receive             0\n",
            "word_freq_will                0\n",
            "word_freq_people              0\n",
            "word_freq_report              0\n",
            "word_freq_addresses           0\n",
            "word_freq_free                0\n",
            "word_freq_business            0\n",
            "word_freq_email               0\n",
            "word_freq_you                 0\n",
            "word_freq_credit              0\n",
            "word_freq_your                0\n",
            "word_freq_font                0\n",
            "word_freq_000                 0\n",
            "word_freq_money               0\n",
            "word_freq_hp                  0\n",
            "word_freq_hpl                 0\n",
            "word_freq_george              0\n",
            "word_freq_650                 0\n",
            "word_freq_lab                 0\n",
            "word_freq_labs                0\n",
            "word_freq_telnet              0\n",
            "word_freq_857                 0\n",
            "word_freq_data                0\n",
            "word_freq_415                 0\n",
            "word_freq_85                  0\n",
            "word_freq_technology          0\n",
            "word_freq_1999                0\n",
            "word_freq_parts               0\n",
            "word_freq_pm                  0\n",
            "word_freq_direct              0\n",
            "word_freq_cs                  0\n",
            "word_freq_meeting             0\n",
            "word_freq_original            0\n",
            "word_freq_project             0\n",
            "word_freq_re                  0\n",
            "word_freq_edu                 0\n",
            "word_freq_table               0\n",
            "word_freq_conference          0\n",
            "char_freq_;                   0\n",
            "char_freq_(                   0\n",
            "char_freq_[                   0\n",
            "char_freq_!                   0\n",
            "char_freq_$                   0\n",
            "char_freq_#                   0\n",
            "capital_run_length_average    0\n",
            "capital_run_length_longest    0\n",
            "capital_run_length_total      0\n",
            "dtype: int64 \n",
            "\n",
            "Values in y: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "missing_values_X = X.isnull().sum()\n",
        "missing_values_y = y.isnull().sum()\n",
        "\n",
        "# Print the results\n",
        "print(\"Values in X:\\n\\n\", missing_values_X, \"\\n\")\n",
        "print(\"Values in y:\", missing_values_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "We will now test if the linear model would still work if we used less data. By using the `train_test_split` function from sklearn, we will create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_small: (4370, 57)\n",
            "y_small: (4370,)\n",
            "X_test: (230, 57)\n",
            "y_test: (230,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets, with 5% of the data as the test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)\n",
        "\n",
        "# Assign the smaller dataset to X_small and y_small\n",
        "X_small = X_train\n",
        "y_small = y_train\n",
        "\n",
        "# Print for testing the shapes of X_small, y_small, X_test, and y_test\n",
        "print(\"X_small:\", X_small.shape)\n",
        "print(\"y_small:\", y_small.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Importing `LogisticRegression` from sklearn\n",
        "2. Instantiating model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implementing the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculating the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "## **Step 5:** Visualize Results\n",
        "\n",
        "1. Creating a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Adding the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Printing `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           Data size  Training accuracy  Validation accuracy\n",
            "0   Complete dataset           0.931957             0.939130\n",
            "1  First two columns           0.616304             0.578261\n",
            "2      Small dataset           0.929748             0.943478\n"
          ]
        }
      ],
      "source": [
        "# Importing LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dataset 1: Complete dataset\n",
        "model_1 = LogisticRegression(max_iter=2000)\n",
        "model_1.fit(X, y)\n",
        "\n",
        "# Dataset 2: Only first two columns of X\n",
        "X_first_two = X.iloc[:, :2]\n",
        "model_2 = LogisticRegression(max_iter=2000)\n",
        "model_2.fit(X_first_two, y)\n",
        "\n",
        "# Dataset 3: X_small and y_small\n",
        "model_3 = LogisticRegression(max_iter=2000)\n",
        "model_3.fit(X_small, y_small)\n",
        "\n",
        "# Calculate the training accuracy for each model\n",
        "train_accuracy_1 = model_1.score(X, y)\n",
        "train_accuracy_2 = model_2.score(X_first_two, y)\n",
        "train_accuracy_3 = model_3.score(X_small, y_small)\n",
        "\n",
        "# Calculate the validation accuracy for each model\n",
        "val_accuracy_1 = model_1.score(X_test, y_test)\n",
        "val_accuracy_2 = model_2.score(X_test.iloc[:, :2], y_test)\n",
        "val_accuracy_3 = model_3.score(X_test, y_test)\n",
        "\n",
        "# Dataframe for storing results\n",
        "results = pd.DataFrame(columns=['Data size', 'Training accuracy', 'Validation accuracy'])\n",
        "\n",
        "# Add the data size, training and validation accuracy for each dataset to the 'results' DataFrame\n",
        "results.loc[0] = ['Complete dataset', train_accuracy_1, val_accuracy_1]\n",
        "results.loc[1] = ['First two columns', train_accuracy_2, val_accuracy_2]\n",
        "results.loc[2] = ['Small dataset', train_accuracy_3, val_accuracy_3]\n",
        "\n",
        "# Print 'results'\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "## **Practice descriptive questions & answers for theoratical understanding**\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "\n",
        "<font color='Green'><b>\n",
        "\n",
        "1. For the first \"complete dataset\", both training and validation accuracies are high, 0.932 and 0.939, respectively. This shows that the model learnt well and generalizes well to the new data that was provided and could potentially be better at interpretting the complex data.\n",
        "\n",
        "    However, the dataset with \"the first two columns\" shows a great decrease in accuracy for both training, 0.616, and validation, 0.578. Here, the validation accuracy is worse and these accuracies imply that decreasing the feature space, reducing the columns, has resulted in a loss of CRITICAL information important for generating correct predictions.\n",
        "\n",
        "    As for the Accuracy for the \"small dataset\", it is somewhat lower than the \"complete dataset\", but is still quite good surprisingly, 0.929 for training and 0.943 for validation. This shows us that even with less data, this particular model is able to perform well because this smaller dataset still contains sufficient information for the model to learn effectively, specially for this type of data.\n",
        "\n",
        "    It is important to note that the accuracy of the model is not only dependent on the amount of data, but also on the quality of the data and the model itself so it is possible that the model is able to learn well with less data because the data is of high quality and the model is well suited for the data.\n",
        "\n",
        "2. A false positive in this spam classification represent when an email that is not spam is wrongly labelled as spam. This could be the result in important emails being ignored or overlooked if they are incorrectly screened. Whereas, a false negative indicates that a spam email was wrongly identified as not spam which then allows it to enter the inbox. Therefore, this does expose the users to a variety of factors including potential frauds or phishing attempts etc. So in this dataset, a false negative is worse than a false positive because it exposes the user to potential harms.\n",
        "\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "## **Process Description/How code was written/Sourcing etc**\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe687f",
      "metadata": {
        "id": "59fe687f"
      },
      "source": [
        "<font color='Green'><b>\n",
        "\n",
        "Using the above written practice questions as guidance, the following layout is used to describe the process:\n",
        "1. The code I used in this Jupyter Notebook was sourced or writen by me and the existing cells information, where necessary.\n",
        "\n",
        "2. As per the professional steps carried out in real-world, I filled/completed these cells in the following order:\n",
        "    - Imported necessary libraries and modules for the code\n",
        "    - Loaded the dataset using the `load_spam()` function from the yellowbrick library\n",
        "    - Data processing and handling missing values was done using the `train_test_split` function from sklearn\n",
        "    - Splitted the data into training and testing sets\n",
        "    - Implemented machine learning models for `Logistic Regression`\n",
        "    - Validated the models and calculated training and validation accuracies\n",
        "    - Visualized the results in a pandas DataFrame/table\n",
        "    - Answered related practice questions to the results and the processes\n",
        "\n",
        "3. No, I did not use generative AI for this. I manually wrote it based on the requirements.\n",
        "\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "# **Part 2: Regression**\n",
        "\n",
        "| **Question**                               |\n",
        "|--------------------------------------------|\n",
        "| **Part 2: Regression**                     |\n",
        "| Step 1: Data Input                         |\n",
        "| Step 2: Data Processing                    |\n",
        "| Step 3: Implement Machine Learning   Model |\n",
        "| Step 4: Validate Mode                      | \n",
        "| Step 5: Visualize Results                  | \n",
        "| Questions                                  | \n",
        "| Process Description                        | \n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. We will be using the `Concrete` dataset from the yellowbrick library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "## **Step 1:** Data Input\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Using the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y` and printing the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "6ff2e34f",
      "metadata": {
        "id": "6ff2e34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of X: (1030, 8)\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y: (1030,)\n",
            "Type of y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Import the dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_concrete\n",
        "X, y = load_concrete()\n",
        "\n",
        "# Print the size and type of X and y\n",
        "print(\"Size of X:\", X.shape)\n",
        "print(\"Type of X:\", type(X))\n",
        "print(\"Size of y:\", y.shape)\n",
        "print(\"Type of y:\", type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "## **Step 2:** Data Processing\n",
        "\n",
        "Checking to see if there are any missing values in the dataset. If necessary, selecting an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "693c5fa3",
      "metadata": {
        "id": "693c5fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values in X:\n",
            "\n",
            " cement    0\n",
            "slag      0\n",
            "ash       0\n",
            "water     0\n",
            "splast    0\n",
            "coarse    0\n",
            "fine      0\n",
            "age       0\n",
            "dtype: int64 \n",
            "\n",
            "Values in y: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "missing_values_X = X.isnull().sum()\n",
        "missing_values_y = y.isnull().sum()\n",
        "\n",
        "# Print the results\n",
        "print(\"Values in X:\\n\\n\", missing_values_X, \"\\n\")\n",
        "print(\"Values in y:\", missing_values_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Importing `LinearRegression` from sklearn\n",
        "2. Instantiating model `LinearRegression()`.\n",
        "3. Implementing the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "b5041945",
      "metadata": {
        "id": "b5041945"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import LinearRegression from sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # 20% of the data is used for testing here\n",
        "\n",
        "# Instantiate the LinearRegression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculating the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "970c038b",
      "metadata": {
        "id": "970c038b"
      },
      "outputs": [],
      "source": [
        "# Import required metrics from sklearn\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Predictions for training and testing sets\n",
        "train_predictions = model.predict(X_train)\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE and R2 for both training and testing sets\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "## **Step 5:** Visualize Results\n",
        "1. Creating a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Adding the accuracy results to the `results` DataFrame\n",
        "3. Printing `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "88d223f3",
      "metadata": {
        "id": "88d223f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Training accuracy Validation accuracy\n",
            "MSE             110.345501           95.635335\n",
            "R2 score          0.609071            0.636898\n"
          ]
        }
      ],
      "source": [
        "# Create a pandas DataFrame 'results' with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "results = pd.DataFrame(index=['MSE', 'R2 score'], columns=['Training accuracy', 'Validation accuracy'])\n",
        "\n",
        "# Add the accuracy results to the 'results' DataFrame\n",
        "results.loc['MSE', 'Training accuracy'] = train_mse\n",
        "results.loc['MSE', 'Validation accuracy'] = test_mse\n",
        "results.loc['R2 score', 'Training accuracy'] = train_r2\n",
        "results.loc['R2 score', 'Validation accuracy'] = test_r2\n",
        "\n",
        "# Print 'results'\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "## **Practice descriptive questions & answers for theoratical understanding**\n",
        "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
        "\n",
        "<font color='Green'><b>\n",
        "\n",
        "Since it predicts continuous variables and evaluates based on how well model can forecast the actual numeric values, the Mean Squared Error (MSE) and R2 score were used to evaluate the performances. And the MSE values are 110.346 for training and 95.635 for validation, with R2 scores of 0.609 and 0.637, respectively.\n",
        "\n",
        "While the R2 scores suggest that the model explains approximately 61-64% of the variance in the target variable, there is a room for improvement since a higher R2 score would indicate a better fit. The model appears to be moderately effective but not highly precise. this suggests that while a linear model captures some of the underlying patterns, the complexity of the \"concrete\" strength dataset might require more complex models or maybe feature engineering to improve accuracy. \n",
        "\n",
        "The almost similar performance on both training and validation sets suggests the model is potentially not overfitting and this is a good thing here at least. However, if you observe more then you can see that the MSE values are quite high. Thus, this suggests that the model is not very accurate, and that the predictions are not very close to the actual values and the linear model did not produce good results for this dataset.\n",
        "\n",
        "</b></font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "## **Process Description/How code was written/Sourcing etc**\n",
        "\n",
        "1. Where did you source your code?\n",
        "2. In what order did you complete the steps?\n",
        "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "<font color='Green'><b>\n",
        "\n",
        "Using the above written practice questions as guidance, the following layout is used to describe the process:\n",
        "1. The code I used in this Jupyter Notebook was sourced or writen by me and the existing cells information, where necessary.\n",
        "\n",
        "2. As per the professional steps carried out in real-world, I filled/completed these cells in the following order:\n",
        "    - Data was loaded using the `load_concrete()` function from the yellowbrick library\n",
        "    - Printed the size and type of `X` and `y`\n",
        "    - Checked and printed for missing values in the dataset, if any, in data processing\n",
        "    - Implemented machine learning models for `Linear Regression` and split the dataset into training and testing sets\n",
        "    - Trained the model on the training set\n",
        "    - Predicted for training and testing sets and calculated MSE and R2 for both training and testing sets\n",
        "    - Visualized the results in a pandas DataFrame/table\n",
        "    - Answered related practice questions to the results and the processes\n",
        "\n",
        "3. No, I did not use generative AI for this. I manually wrote it based on the requirements.\n",
        "\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ac3eb",
      "metadata": {
        "id": "e72ac3eb"
      },
      "source": [
        "# **Part 3: Observations/Interpretation Practice Question(s)**\n",
        "\n",
        "Q. Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "<font color='Green'><b>\n",
        "\n",
        "For the pattern itself, the results show a reasonable level of accuracy, with R2 values ranging from 0.61 to 0.640 approx, which shows that while the linear model can capture some correlations between characteristics and the target variable, it may not catch all of the details or complexities. The lesser percentage of accuracy can also be an indication of this data having non-linear trends. Even the lecture slides stated that linear models can serve as a reasonable baseline but may not be the best in capturing non-linear interactions without extra feature engineering or the usage of more complicated models. Thus, this gives me an understanding that the assumptions of different models is actually critical for choosing the appropriate modeling approach. Even the results show that the model is not overfitting. But the model is also not very accurate which then suggests that the model is actually underfitting. This is a good indication that the model is not capturing all the underlying patterns in the data and again proves that a more complex model may be required to improve accuracy.\n",
        "\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "# **Part 4: Ridge and Lasso regression**\n",
        "\n",
        "Repeating Part 2 with Ridge and Lasso regression to see if we can improve the accuracy results.\n",
        "\n",
        "**NOTE**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "47623d44",
      "metadata": {
        "id": "47623d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best R2 score (Ridge): 0.6369366906855765 with alpha val: 100.0\n",
            "Best R2 score (Lasso): 0.638873238146232 with alpha val: 9.770099572992246\n",
            "Lasso model performed better with an R2 score of 0.638873238146232.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "# Initialize variables to keep track of the best scores and alphas\n",
        "best_r2_ridge, best_r2_lasso = 0.636898, 0.636898  # Initial values based on previous validation accuracy\n",
        "best_alpha_ridge, best_alpha_lasso = None, None\n",
        "alpha_values = np.logspace(-3, 2, num=100)  # Alpha values from 0.001 to 100\n",
        "\n",
        "# Splitting the dataset outside the loop to avoid redundant splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "for alpha_val in alpha_values:\n",
        "    # Train Ridge model\n",
        "    ridge_model = Ridge(alpha=alpha_val)\n",
        "    ridge_model.fit(X_train, y_train)\n",
        "    r2_ridge = ridge_model.score(X_test, y_test)\n",
        "\n",
        "    # Train Lasso model\n",
        "    lasso_model = Lasso(alpha=alpha_val)\n",
        "    lasso_model.fit(X_train, y_train)\n",
        "    r2_lasso = lasso_model.score(X_test, y_test)\n",
        "\n",
        "    # Update best scores and alphas for Ridge\n",
        "    if r2_ridge > best_r2_ridge:\n",
        "        best_r2_ridge = r2_ridge\n",
        "        best_alpha_ridge = alpha_val\n",
        "\n",
        "    # Update best scores and alphas for Lasso\n",
        "    if r2_lasso > best_r2_lasso:\n",
        "        best_r2_lasso = r2_lasso\n",
        "        best_alpha_lasso = alpha_val\n",
        "\n",
        "# Print the best R2 scores and corresponding alpha values\n",
        "print(f'Best R2 score (Ridge): {best_r2_ridge} with alpha val: {best_alpha_ridge}')\n",
        "print(f'Best R2 score (Lasso): {best_r2_lasso} with alpha val: {best_alpha_lasso}')\n",
        "\n",
        "# Determine which model performed better\n",
        "if best_r2_ridge > best_r2_lasso:\n",
        "    print(f'Ridge model performed better with an R2 score of {best_r2_ridge}.')\n",
        "else:\n",
        "    print(f'Lasso model performed better with an R2 score of {best_r2_lasso}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "<font color='Green'><b>\n",
        "\n",
        "Refer to the code and its output above.\n",
        "\n",
        "</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
